{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes| **Assignment**"
      ],
      "metadata": {
        "id": "gRydju6xFf6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions:** Carefully read each question. Use Google Docs, Microsoft Word, or a similar tool to create a document where you type out each question along with its answer. Save the document as a PDF, and then upload it to the LMS. Please do not zip or archive the files before uploading them. Each question carries 20 marks."
      ],
      "metadata": {
        "id": "uAqm4Gx_FyHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 :** What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "JrNRNXTOF_Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Information Gain is a measure used in Decision Trees to decide which feature (or attribute) to split on at each step of building the tree. It helps determine how well a particular feature separates the training examples according to their target classes."
      ],
      "metadata": {
        "id": "tETuhG6GGIXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases."
      ],
      "metadata": {
        "id": "b9sBUQIAGuNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer: comparison of Gini Impurity and Entropy, the two most common impurity measures\n",
        "\n",
        "**Entropy:** Measures the amount of disorder or uncertainty in the data.\n",
        "\n",
        "**Gini Impurity:** Measures the probability of incorrectly classifying a randomly chosen sample.\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "Entropy comes from information theory ‚Äî it quantifies how much ‚Äúinformation‚Äù is needed to describe the dataset.\n",
        "\n",
        "Gini Impurity comes from probability theory ‚Äî it measures how often a randomly chosen sample would be misclassified if it were labeled randomly based on the class distribution.\n"
      ],
      "metadata": {
        "id": "EQ8aGqITKhCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "bGI3HksULV2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Pre-pruning (also called early stopping) is a technique used to stop the growth of a Decision Tree early‚Äîbefore it becomes too complex and starts overfitting the training data.\n",
        "\n",
        "# 1. Concept\n",
        "\n",
        "Instead of letting the tree grow fully and then trimming it, pre-pruning prevents unnecessary splits during the tree-building process.\n",
        "\n",
        "The idea is to stop splitting a node if the split doesn‚Äôt provide a significant improvement in prediction accuracy or information gain.\n",
        "\n",
        "# 2. Common Pre-Pruning Criteria\n",
        "\n",
        "- A tree may stop splitting when:\n",
        "\n",
        "- The information gain (or impurity reduction) is below a threshold.\n",
        "\n",
        "- The number of samples in a node is too small.\n",
        "\n",
        "- The maximum depth of the tree is reached.\n",
        "\n",
        "- The accuracy improvement after a split is minimal."
      ],
      "metadata": {
        "id": "8O28HudfLYlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access  feature_importances_.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "dmFPKR70MBMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "GtbYKWtHMPoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "QuhB8GIUMevX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** What is a Support Vector Machine (SVM)?\n"
      ],
      "metadata": {
        "id": "E_Z5sqC2MgTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANswer:** A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and sometimes regression tasks. It works by finding the best boundary (or hyperplane) that separates different classes in the data."
      ],
      "metadata": {
        "id": "XkITF23yMsrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "zuu1P80CNBJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The Kernel Trick in Support Vector Machines (SVM) is a mathematical technique that allows the algorithm to handle non-linear data by transforming it into a higher-dimensional space‚Äîwithout explicitly computing that transformation."
      ],
      "metadata": {
        "id": "-EUwnosRNLbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Ft_fiJYJMgHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "0h-d2WVyREOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel: {accuracy_rbf:.4f}\")\n"
      ],
      "metadata": {
        "id": "5VRsIl0-NlD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?"
      ],
      "metadata": {
        "id": "vDoDd_kJNy_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANswer:** The Na√Øve Bayes classifier is a probabilistic machine learning algorithm used for classification tasks. It is based on Bayes‚Äô Theorem, which calculates the probability of a class given the observed features.\n",
        "\n",
        "# 1. Bayes‚Äô Theorem\n",
        "**ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")**\n",
        "\n",
        "# 2. Why is it ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "It is called na√Øve because it assumes all features are independent of each other, given the class label. In real-world data, features are often correlated, so this assumption is ‚Äúna√Øve.‚Äù\n",
        "\n",
        "Despite this strong assumption, Na√Øve Bayes often works surprisingly well in practice, especially for text classification (like spam detection or sentiment analysis)."
      ],
      "metadata": {
        "id": "81CWvuhpN7Vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes"
      ],
      "metadata": {
        "id": "1pRmVJhbPCLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "# 1. Gaussian Na√Øve Bayes\n",
        "\n",
        "- Use case: Continuous numerical data (e.g., height, weight, temperature).\n",
        "\n",
        "- Assumption: Features follow a Gaussian (normal) distribution.\n",
        "\n",
        "- How it works: Estimates the likelihood of a feature using the Gaussian probability density function:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "=1\n",
        "2\n",
        "ùúã\n",
        "ùúé\n",
        "ùê∂\n",
        "2\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùúá\n",
        "ùê∂\n",
        ")\n",
        "2\n",
        "2\n",
        "ùúé\n",
        "ùê∂\n",
        "2\n",
        ")\n",
        "- Example: Predicting whether a person has a disease based on continuous measurements like blood pressure or cholesterol levels.\n",
        "\n",
        "# 2. Multinomial Na√Øve Bayes\n",
        "\n",
        "- Use case: Discrete count data, commonly used for text classification.\n",
        "\n",
        "- Assumption: Features represent counts or frequencies (like word counts in a document).\n",
        "\n",
        "- How it works: Computes probability based on the frequency of each feature in the class.\n",
        "\n",
        "- Example: Classifying emails as spam or not based on word counts.\n",
        "\n",
        "# 3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "- Use case: Binary/Boolean features (0 or 1).\n",
        "\n",
        "- Assumption: Features are present or absent, not counts.\n",
        "\n",
        "- How it works: Models each feature as a Bernoulli random variable:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "=ùëù\n",
        "ùëñ\n",
        "ùë•\n",
        "ùëñ\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "1\n",
        "‚àí\n",
        "ùë•\n",
        "ùëñ\n",
        "\n",
        "- Example: Spam detection using binary indicators like ‚Äúcontains word ‚Äòfree‚Äô or not.‚Äù"
      ],
      "metadata": {
        "id": "EjDAdBaOPSt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** **Breast Cancer Dataset** Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "0yQdOuivQxV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "ySDSej54RAXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "EWUazzhSRB7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}