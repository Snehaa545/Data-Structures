{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bagging & Boosting KNN & Stacking Assignment\n"
      ],
      "metadata": {
        "id": "ZpGkD5Ax42dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions:** Carefully read each question. Use Google Docs, Microsoft Word, or a similar tool to create a document where you type out each question along with its answer. Save the document as a PDF, and then upload it to the LMS. Please do not zip or archive the files before uploading them. Each question carries 20 marks.\n"
      ],
      "metadata": {
        "id": "8DW7Kgeq47UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 :** What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?"
      ],
      "metadata": {
        "id": "ZW0-rrWa5J5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The fundamental idea behind ensemble techniques is to combine multiple models (often called “weak learners”) to create a stronger overall model that performs better than any single one of them. The goal is to reduce errors by leveraging the diversity among models — each model compensates for the others’ weaknesses.\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):** Builds multiple models independently using different random subsets of data (created by sampling with replacement).\n",
        "\n",
        "Aims to reduce variance by averaging multiple models to make predictions more stable.\n",
        "\n",
        "**Boosting:** Builds models sequentially, where each new model focuses on correcting the errors made by the previous ones.\n",
        "\n",
        "Aims to reduce bias by giving more weight to misclassified samples and refining the model iteratively."
      ],
      "metadata": {
        "id": "Mbq_ouAN5Y-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.\n"
      ],
      "metadata": {
        "id": "v6J7lMOk6hq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** A Random Forest Classifier reduces overfitting by combining the predictions of many decision trees rather than relying on a single one.\n",
        "\n",
        "A single decision tree tends to overfit because it learns all patterns — even noise — from the training data, making it too specific and less generalizable. Random Forest overcomes this by introducing randomness in both data and features, ensuring that each tree learns slightly different patterns. When their predictions are averaged (or voted), the overall model becomes more stable and less prone to overfitting.\n",
        "\n",
        "**Two Key Hyperparameters and Their Roles**\n",
        "\n",
        "1. n_estimators (number of trees):\n",
        "\n",
        "- More trees mean better averaging and smoother predictions, which reduces variance and overfitting.\n",
        "\n",
        "- However, too many trees can increase training time without much gain after a point.\n",
        "\n",
        "2. max_features (number of features considered at each split):\n",
        "\n",
        "- Controls randomness and correlation among trees.\n",
        "\n",
        "- A smaller value increases diversity (since trees see different features), which helps reduce overfitting.\n",
        "\n",
        "- A larger value makes trees more similar, which can increase overfitting."
      ],
      "metadata": {
        "id": "A1dV4lLg6-XY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.\n"
      ],
      "metadata": {
        "id": "eEG-9uT5-7nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Stacking (Stacked Generalization) is an ensemble learning technique where multiple different models (called base learners) are trained on the same dataset, and their outputs are combined using another model (called a meta-learner or blender) to make the final prediction.\n",
        "\n",
        "The main idea is to let the meta-learner discover how best to combine the strengths of the base models to improve accuracy.\n",
        "\n",
        "**Stacking:** Combines different types of models (e.g., Logistic Regression, Decision Tree, SVM).\n",
        "\n",
        "Base models are trained in parallel, and their predictions are used to train a meta-model.\n",
        "**Bagging:** Uses the same type of model repeatedly (e.g., many Decision Trees).\n",
        "\n",
        "Models are trained independently on random data subsets.\n",
        "\n",
        "**Boosting:** Also uses the same base model but improves it sequentially.\n",
        "\n",
        "Models are trained sequentially, each focusing on correcting errors from the previous one."
      ],
      "metadata": {
        "id": "0aT0i6ll_Fat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?"
      ],
      "metadata": {
        "id": "37z_1wtIBI-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The OOB (Out-of-Bag) Score in a Random Forest is an internal validation score that estimates the model’s performance without needing a separate validation set.\n",
        "\n",
        "\n",
        "When building a Random Forest:\n",
        "\n",
        "- Each tree is trained on a bootstrapped sample — a random sample (with replacement) from the training data.\n",
        "\n",
        "- On average, about 63% of the data points are used to train that tree, and the remaining 37% are not included in that sample. These unused data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "After all trees are built:\n",
        "\n",
        "- Each observation is predicted only by the trees where it was OOB (not used in training).\n",
        "\n",
        "- The OOB Score is then calculated by comparing these OOB predictions with the actual labels."
      ],
      "metadata": {
        "id": "kNrClst8Bius"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Compare AdaBoost and Gradient Boosting in terms of:\n",
        "\n",
        "● How they handle errors from weak learners\n",
        "\n",
        "● Weight adjustment mechanism\n",
        "\n",
        "● Typical use cases"
      ],
      "metadata": {
        "id": "Nbeujr5TCfSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** 1. How They Handle Errors from Weak Learners\n",
        "\n",
        "- **AdaBoost:**\n",
        "Focuses on misclassified samples.After each round, it increases the weights of the incorrectly predicted data points so the next weak learner pays more attention to those errors.\n",
        "\n",
        "- **Gradient Boosting:**\n",
        "Focuses on residual errors (the difference between predicted and actual values).Each new model is trained to predict these residuals, gradually minimizing the overall loss function.\n",
        "\n",
        "2. Weight Adjustment Mechanism\n",
        "\n",
        "- **AdaBoost:**\n",
        "\n",
        "- Assigns weights to each training sample.\n",
        "\n",
        "- After each iteration:\n",
        "\n",
        "- Increases weights for misclassified samples.\n",
        "\n",
        "- Decreases weights for correctly classified samples.\n",
        "\n",
        "- The final prediction is a weighted sum of all weak learners based on their accuracy.\n",
        "\n",
        "- **Gradient Boosting:**\n",
        "\n",
        "- Does not assign weights to samples directly.\n",
        "\n",
        "- Instead, it fits the next model on the negative gradient of the loss function (which represents errors).\n",
        "\n",
        "- Combines models by adding their predictions with a learning rate to control step size.\n",
        "\n",
        "3. Typical Use Cases\n",
        "\n",
        "- **AdaBoost:**\n",
        "\n",
        "- Works best with simple weak learners like decision stumps (one-level trees).\n",
        "\n",
        "- Used for classification problems, especially when the data is clean and not too noisy.\n",
        "\n",
        "- Example: Spam detection, face recognition.\n",
        "\n",
        "- **Gradient Boosting:**\n",
        "\n",
        "- More flexible and powerful — can optimize various loss functions (classification, regression, ranking).\n",
        "\n",
        "- Commonly used for complex datasets and regression or classification tasks.\n",
        "\n",
        "- Example: Credit risk modeling, customer churn prediction, and many Kaggle competition solutions."
      ],
      "metadata": {
        "id": "H7iVrVjWCpKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables.\n"
      ],
      "metadata": {
        "id": "hYrxVjm5Ichb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** CatBoost performs well on categorical features because it’s specifically designed to handle them natively, without requiring manual preprocessing like one-hot or label encoding.\n",
        "\n",
        "# How CatBoost Handles Categorical Variables\n",
        "\n",
        "1. Uses Target Statistics (Target Encoding with Randomness):\n",
        "\n",
        "- Instead of assigning arbitrary numbers, CatBoost replaces each categorical value with a statistic derived from the target variable (like the average target value for that category).\n",
        "\n",
        "- To avoid overfitting, it uses ordered target encoding, where the encoding for each row is based only on previous rows, never on the same or future ones.\n",
        "\n",
        "- This ensures that no data leakage occurs.\n",
        "\n",
        "2. Combinations of Categorical Features:\n",
        "\n",
        "- CatBoost also creates combinations of categorical features (like City + ProductType) to capture interactions automatically.\n",
        "\n",
        "3. Efficient Encoding During Training:\n",
        "\n",
        "- All these transformations are done dynamically during training, so you don’t need to manually encode or scale the data."
      ],
      "metadata": {
        "id": "_rnSVQEgIjA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: KNN Classifier Assignment: Wine Dataset Analysis with Optimization Task:**\n",
        "\n",
        "1. Load the Wine dataset (sklearn.datasets.load_wine()).\n",
        "\n",
        "2. Split data into 70% train and 30% test.\n",
        "\n",
        "3. Train a KNN classifier (default K=5) without scaling and evaluate using:\n",
        "\n",
        "a.Accuracy\n",
        "\n",
        "b. Precision, Recall, F1-Score (print classification report)\n",
        "\n",
        "4. Apply StandardScaler, retrain KNN, and compare metrics.\n",
        "\n",
        "5. Use GridSearchCV to find the best K (test K=1 to 20) and distance metric (Euclidean, Manhattan).\n",
        "\n",
        "6. Train the optimized KNN and compare results with the unscaled/scaled versions.\n"
      ],
      "metadata": {
        "id": "XQ0emXCjJjjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Train KNN (default K=5) without scaling\n",
        "knn_default = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_default.fit(X_train, y_train)\n",
        "y_pred_default = knn_default.predict(X_test)\n",
        "\n",
        "print(\"=== KNN without Scaling ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_default))\n",
        "\n",
        "# Step 5: Apply StandardScaler and retrain KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n=== KNN with StandardScaler ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scaled))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_scaled))\n",
        "\n",
        "# Step 6: Use GridSearchCV to find best K (1–20) and distance metric\n",
        "param_grid = {\n",
        "    'n_neighbors': range(1, 21),\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters from GridSearchCV:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Step 7: Train optimized KNN model\n",
        "best_knn = grid_search.best_estimator_\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n=== Optimized KNN (with Scaling) ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best))\n",
        "\n",
        "# Optional comparison summary\n",
        "results = pd.Data\n"
      ],
      "metadata": {
        "id": "YpIKecboKWMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8 : PCA + KNN with Variance Analysis and Visualization Task:**\n",
        "\n",
        "1. Load the Breast Cancer dataset (sklearn.datasets.load_breast_cance ()).\n",
        "\n",
        "2. Apply PCA and plot the scree plot (explained variance ratio).\n",
        "\n",
        "3. Retain 95% variance and transform the dataset.\n",
        "\n",
        "4. Train KNN on the original data and PCA-transformed data, then compare accuracy.\n",
        "\n",
        "5. Visualize the first two principal components using a scatter plot (color by class).\n"
      ],
      "metadata": {
        "id": "NShTS2QOKwTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "print(\"Dataset Shape:\", X.shape)\n",
        "\n",
        "# Step 3: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Apply PCA and plot Scree Plot (Explained Variance Ratio)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
        "plt.title(\"Scree Plot - Cumulative Explained Variance\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
        "plt.grid(Tr\n"
      ],
      "metadata": {
        "id": "wt7deAErLL2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: KNN Regressor with Distance Metrics and K-Value Analysis Task:**\n",
        "\n",
        "1. Generate a synthetic regression dataset (sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
        "\n",
        "2. Train a KNN regressor with:\n",
        "\n",
        "a. Euclidean distance (K=5)\n",
        "\n",
        "b. Manhattan distance (K=5)\n",
        "\n",
        "c. Compare Mean Squared Error (MSE) for both.\n",
        "\n",
        "3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff.\n"
      ],
      "metadata": {
        "id": "uKp-DlKlLFq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=10, noise=10, random_state=42)\n",
        "\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3a: Train KNN Regressor (K=5, Euclidean distance)\n",
        "knn_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "mse_euclidean = mean_squared_error(y_test, y_pred_euclidean)\n",
        "\n",
        "# Step 3b: Train KNN Regressor (K=5, Manhattan distance)\n",
        "knn_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "mse_manhattan = mean_squared_error(y_test, y_pred_manhattan)\n",
        "\n",
        "# Compare MSE for both distance metrics\n",
        "print(\"=== Distance Metric Comparison (K=5) ===\")\n",
        "print(f\"Euclidean Distance MSE: {mse_euclidean:.4f}\")\n",
        "print(f\"Manhattan Distance MSE: {mse_manhattan:.4f}\")\n",
        "\n",
        "# Step 4: Test K values and analyze bias-variance tradeoff\n",
        "k_values = [1, 5, 10, 20, 50]\n",
        "mse_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean')\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Step 5: Plot K vs MSE\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, mse_scores, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"KNN Regressor: K vs Mean Squared Error\")\n",
        "plt.xlabel(\"Number of Neighbors (K)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JPHVgbmwLua9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data Task:**\n",
        "\n",
        "1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "\n",
        "2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
        "\n",
        "3. Train KNN using:\n",
        "\n",
        "a. Brute-force method\n",
        "\n",
        "b. KD-Tree\n",
        "\n",
        "c. Ball Tree\n",
        "\n",
        "4. Compare their training time and accuracy.\n",
        "\n",
        "5. Plot the decision boundary for the best-performing method (use 2 most important features).\n",
        "Dataset: Pima Indians Diabetes\n",
        "\n"
      ],
      "metadata": {
        "id": "VdCcMK97MkOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 2: Load the Pima Indians Diabetes dataset\n",
        "# If you have the file locally: df = pd.read_csv(\"pima-indians-diabetes.csv\")\n",
        "# Otherwise, you can use a known open-source link:\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
        "           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Replace zero values (invalid) with NaN for imputation\n",
        "cols_with_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "df[cols_with_missing] = df[cols_with_missing].replace(0, np.nan)\n",
        "\n",
        "print(\"\\nMissing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 4: Apply KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df_imputed.isnull().sum())\n",
        "\n",
        "# Step 5: Split dataset\n",
        "X = df_imputed.drop('Outcome', axis=1)\n",
        "y = df_imputed['Outcome']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 6: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train KNN using different algorithms and compare\n",
        "methods = ['brute', 'kd_tree', 'ball_tree']\n",
        "results = []\n",
        "\n",
        "for method in methods:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=method)\n",
        "\n",
        "    start = time.time()\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    train_time = end - start\n",
        "\n",
        "    results.append({'Algorithm': method, 'Accuracy': acc, 'Training Time (s)': train_time})\n",
        "    print(f\"\\n=== {method.upper()} METHOD ===\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Training Time: {train_time:.4f} seconds\")\n",
        "\n",
        "# Step 8: Compare results in a table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Comparison Summary ===\")\n",
        "print(results_df)\n",
        "\n",
        "# Step 9: Choose the best-performing method\n",
        "best_method = results_df.sort_values(by='Accuracy', ascending=False).iloc[0]['Algorithm']\n",
        "print(f\"\\nBest-performing method: {best_method.upper()}\")\n",
        "\n",
        "# Step 10: Visualize decision boundary using top 2 PCA components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_best = KNeighborsClassifier(n_neighbors=5, algorithm=best_method)\n",
        "knn_best.fit(X_pca, y_train)\n",
        "\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
        "                     np.arange(y_min, y_max, 0.05))\n",
        "\n",
        "Z = knn_best.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "plt.title(f\"Decision Boundary using {best_method.upper()} (Top 2 PCA Features)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RU7eIWorM8-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}