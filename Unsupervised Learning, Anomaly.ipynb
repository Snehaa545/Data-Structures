{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning, Anomaly Detection, and Temporal Analysis"
      ],
      "metadata": {
        "id": "MusM7GUozU5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions:** Carefully read each question. Use Google Docs, Microsoft Word, or a similar tool to create a document where you type out each question along with its answer. Save the document as a PDF, and then upload it to the LMS. Please do not zip or  archive the files before uploading them. Each question carries 20 marks."
      ],
      "metadata": {
        "id": "5tYb_3UVzZvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 :** What is Dimensionality Reduction? Why is it important in machine learning?"
      ],
      "metadata": {
        "id": "dkw2YiD40BD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer :** Dimensionality Reduction is the process of reducing the number of input variables (features) in a dataset while keeping as much relevant information as possible. In simple terms, it means simplifying the data by removing less important or redundant features.\n",
        "\n",
        "**Why It’s Important in Machine Learning:**\n",
        "\n",
        "**1. Reduces Overfitting:**\n",
        "Fewer features mean less noise, which helps the model generalize better instead of memorizing the training data.\n",
        "\n",
        "**2. Improves Training Speed:**\n",
        "With fewer dimensions, algorithms require less computation time and memory.\n",
        "\n",
        "**3. Enhances Visualization:**\n",
        "It’s easier to visualize and understand data when it’s reduced to 2D or 3D.\n",
        "\n",
        "**4. Removes Multicollinearity:**\n",
        "Redundant features that are highly correlated can confuse models; dimensionality reduction helps eliminate them.\n",
        "\n",
        "**5. Improves Model Performance:**\n",
        "By focusing on the most informative features, the model can often achieve higher accuracy."
      ],
      "metadata": {
        "id": "AQ77l0Qt0Ncq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Name and briefly describe three common dimensionality reduction techniques.."
      ],
      "metadata": {
        "id": "Hy31KhMG2n1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "1. Principal Component Analysis (PCA)\n",
        "\n",
        "**Type:** Linear technique\n",
        "\n",
        "**Description:**\n",
        "\n",
        "PCA transforms the original correlated features into a smaller set of uncorrelated variables called principal components. These components capture the maximum variance (information) in the data using fewer dimensions.\n",
        "\n",
        "**Use case:** Ideal for numerical data and when you want to preserve as much variance as possible.\n",
        "\n",
        "2. Linear Discriminant Analysis (LDA)\n",
        "\n",
        "**Type:** Supervised technique\n",
        "\n",
        "**Description:**\n",
        "\n",
        "LDA reduces dimensions by finding feature combinations that best separate different classes in the data. It maximizes the distance between class means while minimizing the variation within each class.\n",
        "\n",
        "**Use case:** Commonly used in classification problems (e.g., face recognition).\n",
        "\n",
        "3. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "**Type:** Non-linear technique\n",
        "\n",
        "**Description:**\n",
        "\n",
        "t-SNE focuses on preserving the local structure of data. It converts high-dimensional similarities into probabilities and maps them into a lower-dimensional space (usually 2D or 3D) for visualization.\n",
        "**Use case:** Best for exploring and visualizing complex datasets such as image or text embeddings."
      ],
      "metadata": {
        "id": "VQ7AVOpn2s3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is clustering in unsupervised learning? Mention three popularclustering algorithms."
      ],
      "metadata": {
        "id": "CREWO94x74_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Clustering in unsupervised learning is a technique used to group similar data points together based on their features, without using any predefined labels. The goal is to ensure that data points within the same cluster are more similar to each other than to those in other clusters.\n",
        "\n",
        "#Three Popular Clustering Algorithms:\n",
        "\n",
        "**1. K-Means Clustering**\n",
        "\n",
        "- Divides data into K predefined clusters.\n",
        "\n",
        "- Each point belongs to the cluster with the nearest mean (centroid).\n",
        "\n",
        "- Works best for well-separated, spherical clusters.\n",
        "\n",
        "**2. Hierarchical Clustering**\n",
        "\n",
        "- Builds a hierarchy (tree-like structure) of clusters using a bottom-up (agglomerative) or top-down (divisive) approach.\n",
        "\n",
        "- Results can be visualized with a dendrogram.\n",
        "\n",
        "- No need to predefine the number of clusters.\n",
        "\n",
        "**3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
        "\n",
        "- Groups points that are close in density and labels low-density points as noise or outliers.\n",
        "\n",
        "- Can find clusters of arbitrary shape.\n",
        "\n",
        "- Doesn’t require specifying the number of clusters beforehand."
      ],
      "metadata": {
        "id": "YZL6XIdq8IIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** Explain the concept of anomaly detection and its significance."
      ],
      "metadata": {
        "id": "AV6N7uPP-CGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Anomaly detection is the process of identifying data points, events, or patterns that deviate significantly from the normal behavior in a dataset. These unusual observations are called anomalies or outliers.\n",
        "\n",
        "**Concept:**\n",
        "\n",
        "In most datasets, the majority of data follows a common pattern or distribution. Anomaly detection focuses on spotting the few instances that don’t fit this pattern.\n",
        "\n",
        "For example, if you track daily transactions for a credit card user, most purchases will fall within a typical range. A sudden, unusually large purchase in another country could be flagged as an anomaly.\n",
        "\n",
        "**Significance:**\n",
        "\n",
        "**1. Fraud Detection:**\n",
        "\n",
        "Detects unusual transactions in banking or credit card systems that may indicate fraud.\n",
        "\n",
        "**2. Network Security:**\n",
        "\n",
        "Identifies abnormal network activity that could signal hacking or malware.\n",
        "\n",
        "**3. Quality Control:**\n",
        "\n",
        "Spots defects or irregularities in manufacturing processes.\n",
        "\n",
        "**4. Healthcare Monitoring:**\n",
        "\n",
        "Detects abnormal patterns in patient data, such as irregular heart rates.\n",
        "\n",
        "**5. System Maintenance:**\n",
        "\n",
        "Helps in predicting equipment failures by identifying abnormal sensor readings."
      ],
      "metadata": {
        "id": "lFv2WZy7_BsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** List and briefly describe three types of anomaly detection techniques."
      ],
      "metadata": {
        "id": "PX6nm7nQAtmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. Statistical Methods**\n",
        "\n",
        "**Description:**\n",
        "\n",
        "These methods assume that normal data follows a certain statistical distribution (like Gaussian or normal distribution). Data points that fall far from the expected range are marked as anomalies.\n",
        "Example: Using Z-score or Interquartile Range (IQR) to detect outliers.\n",
        "Use case: Simple and effective for small or well-behaved datasets.\n",
        "\n",
        "**2. Distance-Based Methods**\n",
        "\n",
        "**Description:**\n",
        "\n",
        "These rely on the idea that normal data points are close to their neighbors, while anomalies are far away.\n",
        "\n",
        "Example: K-Nearest Neighbors (KNN) calculates distances between points and flags those far from others as anomalies.\n",
        "\n",
        "**Use case:** Works well when the data structure is geometric (e.g., spatial or numeric data).\n",
        "\n",
        "**3. Machine Learning–Based Methods**\n",
        "\n",
        "**Description:**\n",
        "\n",
        "These techniques learn normal patterns from data and identify instances that don’t fit those learned patterns.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Isolation Forest isolates anomalies instead of profiling normal data.\n",
        "\n",
        "- One-Class SVM separates normal data from outliers using a decision boundary.\n",
        "\n",
        "- Autoencoders (in deep learning) reconstruct normal data well but fail on anomalies.\n",
        "\n",
        "**Use case:** Suitable for complex, high-dimensional, or large-scale datasets."
      ],
      "metadata": {
        "id": "d99z9uxvA1SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** What is time series analysis? Mention two key components of time series data.\n"
      ],
      "metadata": {
        "id": "JoJ7LNxLFJ3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Time series analysis is the process of studying data points collected or recorded over time to identify meaningful patterns, trends, and seasonal behaviors.\n",
        "\n",
        "In simple terms, it helps understand how data changes over time and can be used to make forecasts or predictions about future values.\n",
        "\n",
        "#Two Key Components of Time Series Data:\n",
        "\n",
        "**1. Trend:**\n",
        "The long-term direction or movement in the data over time.\n",
        "\n",
        "- Example: A steady increase in a company’s sales over several years.\n",
        "\n",
        "**2. Seasonality:**\n",
        "Regular and repeating patterns that occur at specific intervals.\n",
        "\n",
        "- Example: Ice cream sales increasing every summer and dropping in winter."
      ],
      "metadata": {
        "id": "Fcd-_GvuFOXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Describe the difference between seasonality and cyclic behavior in time series."
      ],
      "metadata": {
        "id": "rXTYW2_gGGhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "#1. Seasonality\n",
        "\n",
        "- Definition: Regular, repeating patterns that occur at fixed and predictable intervals (like days, months, or quarters).\n",
        "\n",
        "- Duration: Short-term and tied to the calendar.\n",
        "\n",
        "- Cause: Often influenced by factors like weather, holidays, or human behavior.\n",
        "\n",
        "- Example:\n",
        "\n",
        "- Retail sales increasing every December due to the holiday season.\n",
        "\n",
        "- Electricity usage peaking every summer because of air conditioning.\n",
        "\n",
        "#2. Cyclic Behavior\n",
        "\n",
        "- Definition: Fluctuations that occur over longer, irregular periods, often driven by economic or business cycles.\n",
        "\n",
        "- Duration: No fixed or regular interval — cycles vary in length.\n",
        "\n",
        "- Cause: Influenced by broader forces like economic growth, market trends, or social factors.\n",
        "\n",
        "- Example:\n",
        "\n",
        "- A country’s economy expanding and contracting over several years.\n",
        "\n",
        "- Real estate prices rising and falling with market cycles."
      ],
      "metadata": {
        "id": "vg1Xd-FUGTd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write Python code to perform K-means clustering on a sample dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "EVwIaVi5HQ-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Step 1: Create a sample dataset\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=0)\n",
        "\n",
        "# Step 2: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Step 3: Get cluster centers and labels\n",
        "centers = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Step 4: Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "plt.title(\"K-Means Clustering Example\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Print cluster centers\n",
        "print(\"Cluster Centers:\\n\", centers)\n"
      ],
      "metadata": {
        "id": "6uuMJ3xkHnba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** What is inheritance in OOP? Provide a simple example in Python.\n"
      ],
      "metadata": {
        "id": "5V6bcw9kHoW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Inheritance in Object-Oriented Programming (OOP) is a mechanism that allows one class (called the child or derived class) to inherit properties and methods from another class (called the parent or base class).\n",
        "\n",
        "It helps in code reusability, extending functionality, and maintaining a clear class hierarchy."
      ],
      "metadata": {
        "id": "9QPONQHAHzlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent class\n",
        "class Animal:\n",
        "    def speak(self):\n",
        "        print(\"Animals make sounds\")\n",
        "\n",
        "# Child class inheriting from Animal\n",
        "class Dog(Animal):\n",
        "    def speak(self):\n",
        "        print(\"Dog barks\")\n",
        "\n",
        "# Creating objects\n",
        "a = Animal()\n",
        "d = Dog()\n",
        "\n",
        "a.speak()   # Output: Animals make sounds\n",
        "d.speak()   # Output: Dog barks\n"
      ],
      "metadata": {
        "id": "Qg94a9UZH8D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** How can time series analysis be used for anomaly detection?\n"
      ],
      "metadata": {
        "id": "NisC5UysH-3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Time series analysis can be used for anomaly detection by identifying data points or patterns that deviate from the expected time-based behavior. Since time series data changes over time, detecting anomalies means spotting sudden spikes, drops, or irregular patterns that don’t match the usual trend or seasonality.\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "**1. Model Normal Behavior:**\n",
        "\n",
        "A time series model (like ARIMA, LSTM, or Exponential Smoothing) is trained on historical data to understand normal patterns — including trend and seasonality.\n",
        "\n",
        "**2. Predict Expected Values:**\n",
        "\n",
        "The model forecasts future values or reconstructs what normal values should look like.\n",
        "\n",
        "**3. Compare Actual vs. Expected:**\n",
        "\n",
        "The difference (called residual or error) between actual and predicted values is analyzed.\n",
        "If the error exceeds a certain threshold, that point is flagged as an anomaly."
      ],
      "metadata": {
        "id": "HnF-EnnqIJDk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}